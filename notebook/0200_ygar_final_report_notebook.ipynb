{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708621d6-3dfa-46da-a94b-bf9129168a6b",
   "metadata": {},
   "source": [
    "# Action Recognition\n",
    "### Utilizing YGAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e0de0-be8a-4d6d-8bf2-1628b812ff4d",
   "metadata": {},
   "source": [
    "##### Shuo Wang, Amiya Ranjan and Lawrence Jiang\n",
    "##### DATASCI 281 Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb80d473-7902-4e45-a6e7-c114601e97ff",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf29205-2074-4654-87f6-fe3d73634aaf",
   "metadata": {},
   "source": [
    "The scarcity of high quality actions video data is a bottleneck in the research and application of action recognition. Although significant effort has been made in this area, there still exist gaps in the range of available data types in which a more flexible and comprehensive data set could help bridge. In this paper, we present a new process of 3-D actions data simulation engine and generate 3 sets of sample data to demonstrate its current functionalities. With the new data generation process, we demonstrate its applications to image classifications, action recognitions and potential to evolution into a system that would allow the exploration of much more complex action recognition tasks. In order to show off these capabilities, we also train and test a list of commonly used models for image recognition to demonstrate the potential applications and capabilities of the data sets and their generation process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30639e-9a7e-4e24-857f-625b6802b8d3",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3321506-74dc-43c2-93d4-4070ea554120",
   "metadata": {},
   "source": [
    "Action recognition is an important area of research within the field of machine learning. Potential applications of movement recognition and understanding of human actions are endless, ranging from robotics and security surveillance to human-machine interactions. Successful progress in this field would translate to solutions for many real-world problems. Not only are the potentials enormous, diverse distinct research topics exist within the broad category of action recognition. Single action classification is one of the more widely studied topics, and shares many similar techniques to image classification. Moving beyond single actions, localized multiple object action recognition are also possible. Even sequential action recognition has been attempted. Although tremendous progress has been made, much still remains to be done. Some of the obstacles facing the research efforts include: relative scarcity of high quality data, high resource requirements for training potential model architectures and difficulty in the processing and modification of input data for analysis.\n",
    "\n",
    "In this paper, we propose a new method of video actions data generation by means of 3D simulation, where data generation could be customized to facilitate various research goals and specific areas of focus, including single action recogition, action orientation detection and action sequence segmentation, to name a few. In addition, we also perform several tests of action recognition using classic image classification modeling techniques and deep learning techniques, demonstrating how our dataset could be leveraged to bridge the gap between image classification tasks and 3D action recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3849e2-e8f8-4ae9-a1a2-51ad797bebc8",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a3e72-c2e2-49f3-a449-27cf59519264",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb633a3-e6e5-481c-9010-f2a9cc0f2088",
   "metadata": {},
   "source": [
    "Several data sets exist currently for single action recognition, including UCF101 and HMDB. UCF101 dataset is a collection of youtube videos that contains 101 action categories with a total of 13320 videos, averaging 132 videos per category. Some of the categories in the dataset include \"Apply Eye Makeup\", \"archery\" and \"Frisbee catch\".\n",
    "\n",
    "HMDB is another data set that aims to further the progress in understanding action recognition.  It contains 51 actions and a total of 7000 vidoe clips. The actions within the videos are broadly categorized into five types: general facial actions, facial actions with object manipulation, general body movements, body movements with object interaction and body movements for human interaction.\n",
    "\n",
    "Over time, larger data sets have been created for video action recognition tasks such as the kinetics dataset, where 700 classes of actions have been categoried, each with an average of 926 sample videos.\n",
    "\n",
    "More recently, spatio-temporally localized atomic visual actions data sets (AVA) have also been introduced, where actions by multiple objects exist within the same video sample.Over time, larger data sets have been created for video action recognition tasks such as the kinetics dataset\\cite{Smaira:2020qr}, where 700 classes of actions have been categoried, each with an average of 926 sample videos.\n",
    "\n",
    "More recently, spatio-temporally localized atomic visual actions data sets (AVA) have also been introduced, where actions by multiple objects exist within the same video sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91927f9f-a63b-4326-a291-5b75bfc8972a",
   "metadata": {},
   "source": [
    "### Action Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7fe7eb-b30e-4672-87d8-2c4b79bab967",
   "metadata": {},
   "source": [
    "As more and more data sets come into existence, models have been built to train and test on these data sets, including convolutional neural network, LSTM based encoder models and 3D neural networks that combines frames and optical flow information. More complex architectures for localized action recognitions have also been proposed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb51799-e270-4c8c-a7fb-9ac0cb9b52a4",
   "metadata": {},
   "source": [
    "### Bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d21b2-9db3-400a-938b-26291321f362",
   "metadata": {},
   "source": [
    "Although commendable progress has been made in the research of action understanding, many issues still hinder its progress. One of them is the quality of available data sets. The existing data sets are typically collected from video data sources and curated by human judges, this process introduces many variabilities in the quality and comprehensiveness of the sample data, and because these data are collected as they are available, often it is impossible to control for characteristics that are desired for particular research objectiveness. For example, it is often difficult to study the specific effects of object variation and orientation separately. Besides targeted research needs, it is often difficult to study hierarchies of categories. As an example, if we would like to conduct a research where we would like to first recognize the person in the image, and then the action performed by the person, then it would be very difficult to conduct such studies with the existing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb68a63-262f-4edf-9e5a-4661c17c3bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
