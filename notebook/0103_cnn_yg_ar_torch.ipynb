{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b0c144-042d-47aa-b2b6-94b9888d5ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb78370-8250-42f4-9eff-72236f96aece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_label_map(labels):\n",
    "    label_set = set()\n",
    "    for lt in labels:\n",
    "        label_set.add(lt)\n",
    "\n",
    "    label_map = {}\n",
    "    count = 0\n",
    "    for l in label_set:\n",
    "        label_map[l] = count\n",
    "        count += 1\n",
    "        \n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d189fd08-4a09-4414-9540-bc4d8eb46238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=\"same\")\n",
    "        #self.norm2a = nn.BatchNorm2d(32)\n",
    "        self.norm2b = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=\"same\")\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=\"same\")\n",
    "        #self.norm4a = nn.BatchNorm2d(64)\n",
    "        self.norm4b = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=\"same\")\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=\"same\")\n",
    "        #self.conv7 = nn.Conv2d(128, 128, kernel_size=3, padding=\"same\")\n",
    "        #self.norm7a = nn.BatchNorm2d(128)\n",
    "        self.norm7b = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(128, 256, kernel_size=3, padding=\"same\")\n",
    "        self.conv9 = nn.Conv2d(256, 256, kernel_size=3, padding=\"same\")\n",
    "        #self.conv10 = nn.Conv2d(256, 256, kernel_size=3, padding=\"same\")\n",
    "        #self.norm10a = nn.BatchNorm2d(256)\n",
    "        self.norm10b = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv11 = nn.Conv2d(256, 512, kernel_size=3, padding=\"same\")\n",
    "        self.conv12 = nn.Conv2d(512, 512, kernel_size=3, padding=\"same\")\n",
    "        #self.conv13 = nn.Conv2d(512, 512, kernel_size=3, padding=\"same\")\n",
    "        #self.norm13a = nn.BatchNorm2d(512)\n",
    "        self.norm13b = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8192, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #x = self.norm2a(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.norm2b(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        #x = self.norm4a(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.norm4b(x)\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        #x = F.relu(self.conv7(x))\n",
    "        #x = self.norm7a(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.norm7b(x)\n",
    "        \n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        #x = F.relu(self.conv10(x))\n",
    "        #x = self.norm10a(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.norm10b(x)\n",
    "        \n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv12(x))\n",
    "        #x = F.relu(self.conv13(x))\n",
    "        #x = self.norm13a(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.norm13b(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 8192)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4346fd8f-7343-4636-9b25-7205afc59bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "        epoch,\n",
    "        model,\n",
    "        loss_func,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        log_interval,\n",
    "        save_path\n",
    "):  \n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "\n",
    "            trained_count = batch_idx * len(data)\n",
    "            total_count = len(train_loader.dataset)\n",
    "            batch_percent = int(100 * batch_idx / len(train_loader))\n",
    "            loss_val = loss.item() / len(data)\n",
    "            print(\n",
    "                f'Train Epoch: {epoch} ' + \n",
    "                f'[{trained_count}/{total_count} ({batch_percent}%)]' + \n",
    "                f'\\tLoss: {loss_val:.6f}'\n",
    "            )\n",
    "\n",
    "    torch.save(model.state_dict(), osp.join(save_path, f\"model_{epoch}.pt\"))\n",
    "    torch.save(optimizer.state_dict(), osp.join(save_path, f\"opt_{epoch}.pt\"))\n",
    "    \n",
    "    return loss.item() / len(data)\n",
    "\n",
    "\n",
    "def test(\n",
    "    model,\n",
    "    loss_func,\n",
    "    test_loader,\n",
    "    test_type=\"Validation\"\n",
    "):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += loss_func(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        \n",
    "        print(\n",
    "            f'{test_type} Average loss: {test_loss:.4f}, ' +\n",
    "            f'Accuracy: {correct}/{len(test_loader.dataset)} ' + \n",
    "            f'({100.*correct/len(test_loader.dataset):.0f}%)'\n",
    "        )\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc0ce214-6cf9-495b-900d-08950a123aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fad728e34f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size_train = 128\n",
    "batch_size_valid = 128\n",
    "batch_size_test = 128\n",
    "learning_rate = 0.0001\n",
    "log_interval = 20\n",
    "save_path = \"/home/ubuntu/data/yg_ar/cnn_torch/res0002\"\n",
    "\n",
    "if not osp.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "669c236f-053e-42e3-a893-5f190ad9e5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nebula.data.yg_ar.setup_data_image_hard import read_data\n",
    "from nebula.common import to_scale_one, write_pickle, read_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe934af-4abe-4eae-b7a3-c08352cc3398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_path = \"/home/ubuntu/data/yg_ar/image_hard_df.pkl\"\n",
    "random_seed = 1\n",
    "df, train_df, test_df, valid_df = read_data(df_path, random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3191e21f-c513-4e61-b9d9-1907900ad11c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_map_a = create_label_map(df[\"label_a\"])\n",
    "label_map_at = create_label_map(df[\"label_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "446c9584-3c40-405c-b2a5-599f72841fad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_x = train_df[\"image\"].apply(lambda x: np.array([x.astype(np.float32)/225.0])).to_list()\n",
    "train_y_a = train_df[\"label_a\"].map(label_map_a).to_list()\n",
    "train_y_at = train_df[\"label_at\"].map(label_map_at).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3967b8f9-784e-4586-9dae-c72c0aa7814d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_x = valid_df[\"image\"].apply(lambda x:  np.array([x.astype(np.float32)/225.0])).to_list()\n",
    "valid_y_a = valid_df[\"label_a\"].map(label_map_a).to_list()\n",
    "valid_y_at = valid_df[\"label_at\"].map(label_map_at).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9221bfea-71c8-4816-9e18-dd74d126ec8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_x = test_df[\"image\"].apply(lambda x:  np.array([x.astype(np.float32)/225.0])).to_list()\n",
    "test_y_a = test_df[\"label_a\"].map(label_map_a).to_list()\n",
    "test_y_at = test_df[\"label_at\"].map(label_map_at).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab68f1d7-45bd-4849-a8d3-9900fc44dd8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Average loss: 2.3029, Accuracy: 176/1760 (10%)\n",
      "Train Epoch: 1 [0/16080 (0%)]\tLoss: 2.304047\n",
      "Train Epoch: 1 [1280/16080 (7%)]\tLoss: 2.348608\n",
      "Train Epoch: 1 [2560/16080 (15%)]\tLoss: 2.210284\n",
      "Train Epoch: 1 [3840/16080 (23%)]\tLoss: 2.199447\n",
      "Train Epoch: 1 [5120/16080 (31%)]\tLoss: 2.058052\n",
      "Train Epoch: 1 [6400/16080 (39%)]\tLoss: 1.894537\n",
      "Train Epoch: 1 [7680/16080 (47%)]\tLoss: 1.807622\n",
      "Train Epoch: 1 [8960/16080 (55%)]\tLoss: 1.550088\n",
      "Train Epoch: 1 [10240/16080 (63%)]\tLoss: 1.482506\n",
      "Train Epoch: 1 [11520/16080 (71%)]\tLoss: 1.407411\n",
      "Train Epoch: 1 [12800/16080 (79%)]\tLoss: 1.083449\n",
      "Train Epoch: 1 [14080/16080 (87%)]\tLoss: 0.931750\n",
      "Train Epoch: 1 [15360/16080 (95%)]\tLoss: 1.119684\n",
      "Validation Average loss: 1.0630, Accuracy: 1119/1760 (64%)\n",
      "Train Epoch: 2 [0/16080 (0%)]\tLoss: 0.906883\n",
      "Train Epoch: 2 [1280/16080 (7%)]\tLoss: 0.691343\n",
      "Train Epoch: 2 [2560/16080 (15%)]\tLoss: 0.671717\n",
      "Train Epoch: 2 [3840/16080 (23%)]\tLoss: 0.740725\n",
      "Train Epoch: 2 [5120/16080 (31%)]\tLoss: 0.321101\n",
      "Train Epoch: 2 [6400/16080 (39%)]\tLoss: 0.499854\n",
      "Train Epoch: 2 [7680/16080 (47%)]\tLoss: 0.307256\n",
      "Train Epoch: 2 [8960/16080 (55%)]\tLoss: 0.354040\n",
      "Train Epoch: 2 [10240/16080 (63%)]\tLoss: 0.217783\n",
      "Train Epoch: 2 [11520/16080 (71%)]\tLoss: 0.210335\n",
      "Train Epoch: 2 [12800/16080 (79%)]\tLoss: 0.312767\n",
      "Train Epoch: 2 [14080/16080 (87%)]\tLoss: 0.172162\n",
      "Train Epoch: 2 [15360/16080 (95%)]\tLoss: 0.278758\n",
      "Validation Average loss: 0.7012, Accuracy: 1341/1760 (76%)\n",
      "Train Epoch: 3 [0/16080 (0%)]\tLoss: 0.135953\n",
      "Train Epoch: 3 [1280/16080 (7%)]\tLoss: 0.325038\n",
      "Train Epoch: 3 [2560/16080 (15%)]\tLoss: 0.134085\n",
      "Train Epoch: 3 [3840/16080 (23%)]\tLoss: 0.117028\n",
      "Train Epoch: 3 [5120/16080 (31%)]\tLoss: 0.087991\n",
      "Train Epoch: 3 [6400/16080 (39%)]\tLoss: 0.170575\n",
      "Train Epoch: 3 [7680/16080 (47%)]\tLoss: 0.189339\n",
      "Train Epoch: 3 [8960/16080 (55%)]\tLoss: 0.057022\n",
      "Train Epoch: 3 [10240/16080 (63%)]\tLoss: 0.089669\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m test(\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     10\u001b[0m     loss_func\u001b[38;5;241m=\u001b[39mloss_func,\n\u001b[1;32m     11\u001b[0m     test_loader\u001b[38;5;241m=\u001b[39mvalid_loader_a\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 15\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     train_res\u001b[38;5;241m.\u001b[39mappend((epoch, train_loss))\n\u001b[1;32m     26\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m test(\n\u001b[1;32m     27\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     28\u001b[0m         loss_func\u001b[38;5;241m=\u001b[39mloss_func,\n\u001b[1;32m     29\u001b[0m         test_loader\u001b[38;5;241m=\u001b[39mvalid_loader_a\n\u001b[1;32m     30\u001b[0m     )\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, model, loss_func, train_loader, optimizer, log_interval, save_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(output, target)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader_a = torch.utils.data.DataLoader(tuple(zip(train_x, train_y_a)), batch_size=batch_size_train, shuffle=True)\n",
    "train_loader_at = torch.utils.data.DataLoader(tuple(zip(train_x, train_y_at)), batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "valid_loader_a = torch.utils.data.DataLoader(tuple(zip(valid_x, valid_y_a)), batch_size=batch_size_valid, shuffle=True)\n",
    "valid_loader_at = torch.utils.data.DataLoader(tuple(zip(valid_x, valid_y_at)), batch_size=batch_size_valid, shuffle=True)\n",
    "\n",
    "test_loader_a = torch.utils.data.DataLoader(tuple(zip(test_x, test_y_a)), batch_size=batch_size_test, shuffle=True)\n",
    "test_loader_at = torch.utils.data.DataLoader(tuple(zip(test_x, test_y_at)), batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_func = nn.CrossEntropyLoss(size_average=False)\n",
    "\n",
    "train_res = []\n",
    "test_res = []\n",
    "\n",
    "test(\n",
    "    model=model,\n",
    "    loss_func=loss_func,\n",
    "    test_loader=valid_loader_at\n",
    ")\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_loss = train(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        loss_func=loss_func,\n",
    "        train_loader=train_loader_at,\n",
    "        optimizer=optimizer,\n",
    "        log_interval=log_interval,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    train_res.append((epoch, train_loss))\n",
    "\n",
    "    test_loss = test(\n",
    "        model=model,\n",
    "        loss_func=loss_func,\n",
    "        test_loader=valid_loader_at\n",
    "    )\n",
    "    test_res.append((epoch, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58390a0-45da-4e87-a834-37719c413ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
